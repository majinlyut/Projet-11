# Sport Data Solution â€” ETL streaming dâ€™activitÃ©s sportives

## ğŸš€ Objectif
Ce projet implÃ©mente un **pipeline de streaming en temps rÃ©el** pour :
- **Ingestion** en continu des Ã©vÃ©nements sportifs (Strava) 
- **Transformation et enrichissement** via Spark Structured Streaming 
- **Stockage** bdd Postgresql et delta lake
- **Monitoring en temps rÃ©el** avec Prometheus & Grafana
- **Notifications instantanÃ©es** via Slack des activitÃ©s des emplyoÃ©
- **Tableaux de bord** rapport Power BI avec les indicateurs Ã  suivre (primes total,jours bien-Ãªtre...)

---

## ğŸ“‚ Structure du projet
```
.
â”œâ”€â”€ docker-compose.yml        # Orchestration des services
â”œâ”€â”€ requirements.txt          # DÃ©pendances Python
â”œâ”€â”€ maps.py                   # Calcul la distance maison/travail
â”œâ”€â”€ strava_generator.py       # GÃ©nÃ©rateur de flux d'activitÃ©s sportives sur 1 an
â”œâ”€â”€ strava_generator1.py      # GÃ©nÃ©rateur d'une activitÃ© Ã  date pour test
â”œâ”€â”€ validate_strava.py        # Validation qualitÃ© des donnÃ©es
â”œâ”€â”€ config/                   # Config Spark & YAML
â”œâ”€â”€ connect/                  # Docker Kafka Connect + JMX
â”œâ”€â”€ data/                     # DonnÃ©es Excel initiales (RH & sportives)
â”œâ”€â”€ expectations/             # Tests Great Expectations
â”œâ”€â”€ init/                     # Scripts d'initialisation PostgreSQL
â”œâ”€â”€ monitoring/               # Config Prometheus + Grafana
â”œâ”€â”€ slack/                    # Notifications Slack
â”œâ”€â”€ spark/                    # Jobs Spark bronze/gold + mÃ©triques
â”œâ”€â”€ spark_thrift/             # Dockerfile pour Spark Thrift
â””â”€â”€ topic_creator/            # Docker pour crÃ©er les topics Redpanda
```

---

## ğŸ› ï¸ Stack technique
- **Redpanda** pour lâ€™ingestion dâ€™Ã©vÃ©nements en continu
- **Spark Structured Streaming** pour les traitements temps rÃ©el
- **Delta Lake** pour le stockage intermÃ©diaire et la gestion des versions
- **PostgreSQL** comme base relationnelle cible
- **Great Expectations** pour la validation continue de la qualitÃ©
- **Prometheus + Grafana** pour visualiser les mÃ©triques en temps rÃ©el
- **Slack API** pour notifier instantanÃ©ment les utilisateurs

---

## â–¶ï¸ Lancer le pipeline

1. **Cloner le repo**
   ```bash
   git clone <repo>
   cd projet11
   ```

2. **Construire & lancer les services**
   ```bash
   docker-compose up --build
   ```

3. **Lancer le script de calcul de distance Domicile/Travail (Ã  faire 1 seule fois)**
   ```bash
   python maps.py
   ```


4. **DÃ©marrer le connecteur Debezium**
   ```bash
   curl -X POST http://localhost:8083/connectors \
  -H "Content-Type: application/json" \
  -d '{
    "name": "postgres-connector",
    "config": {
      "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
      "plugin.name": "pgoutput",
      "database.hostname": "postgres",
      "database.port": "5432",
      "database.user": "user",
      "database.password": "password",
      "database.dbname": "sportsdb",
      "topic.prefix": "sportsdata",
      "table.include.list": "public.sport_activities",
      "publication.name": "debezium_publication",
      "slot.name": "debezium_slot",
      "tombstones.on.delete": "false"
    }
  }'

   ```

5. **DÃ©marrer le flux de donnÃ©es**
   ```bash
   python strava_generator.py
   ```

6. **VÃ©rifier la qualitÃ© des donnÃ©es**
   ```bash
   python validate_strava.py
   ```

## ğŸ“Š Monitoring en temps rÃ©el
- **Prometheus** : `http://localhost:9090`
- **Grafana** : `http://localhost:3000` (importer `monitoring/grafana.json` pour visualiser les dashboards)
---
## ğŸ“Š Architecture

<img width="960" height="308" alt="image" src="https://github.com/user-attachments/assets/1c2d4783-fb6e-4949-a395-9d7e6674ac7d" />


1-Les donnÃ©es sont importÃ©es dans la BDD postgresql
2-Debezium rÃ©cupÃ¨re le delta de la base(insertion,modification,supression)
3-Redpanda git comme bus de donnÃ©es temps rÃ©el, assurant la mise en file et la diffusion scalable vers les consommateurs 
4-Le consommateur python publie un message de nouvelle activitÃ© sur le canal Slack de l'Ã©quipe
5-Le consommateur spark streaming Ã©crit les donnÃ©es brut dans un deltalake bronze
6-Un second job Spark tranforme les donnÃ©es et les agrÃ¨ges avec les donnÃ©es RH puis Ã©crit dans un delta lake Gold
7-Les donnÃ©es enrichies et fiables sont prÃªtes Ã  Ãªtre exploitÃ© par Power BI ou exports.

## ğŸ“Š Monitoring en temps rÃ©el
- **Prometheus** : `http://localhost:9090`
- **Grafana** : `http://localhost:3000` (importer `monitoring/grafana.json` pour visualiser les dashboards)




---

