#!/usr/bin/env python3
import os
import time
import yaml
import hashlib
import pandas as pd
from pyspark.sql import SparkSession, functions as F
from pyspark.sql.streaming import StreamingQuery

# â€”â€”â€” Chemins Delta â€”â€”â€”
DELTA_BRONZE_PATH = "/delta/bronze/sport_activities"
DELTA_GOLD_PATH   = "/delta/gold/eligibilites"
CHECKPOINT_PATH   = "/delta/checkpoints/gold_streaming"
HASH_FILE         = "/delta/last_config_hash.txt"

# â€”â€”â€” Spark â€”â€”â€”
spark = SparkSession.builder.appName("GoldStreaming").getOrCreate()

# â€”â€”â€” Charger config + hash â€”â€”â€”
def load_config_and_hash():
    with open("/config/config.yml", "rb") as f:
        raw = f.read()
    return yaml.safe_load(raw), hashlib.md5(raw).hexdigest()

# VÃ©rification hash au dÃ©marrage
cfg, current_hash = load_config_and_hash()
last_hash = open(HASH_FILE).read().strip() if os.path.exists(HASH_FILE) else None
if last_hash != current_hash:
    print("ğŸ”¥ Config modifiÃ©e depuis le dernier run â†’ suppression Gold.")
    import shutil
    shutil.rmtree(DELTA_GOLD_PATH, ignore_errors=True)
    with open(HASH_FILE, "w") as f:
        f.write(current_hash)

# â€”â€”â€” RH + BU + Distance â€”â€”â€”
rh_pdf = pd.read_excel("/data/DonneÌesRH.xlsx")
rh_pdf.columns = rh_pdf.columns.str.strip()
ref_rh = (
    spark.createDataFrame(rh_pdf)
         .selectExpr(
             "cast(`ID salariÃ©` as long) as employee_id",
             "`BU` as bu",
             "`Moyen de dÃ©placement` as moyen_dep",
             "cast(`Salaire brut` as double) as salaire_brut_annuel",
             "cast(`distance_km` as double) as distance_km"
         )
)

# â€”â€”â€” Fonction de calcul Gold (rÃ©utilisable pour batch initial et streaming) â€”â€”â€”
def build_gold_from_bronze(full_bronze, cfg):
    MIN_ACTIVS = int(cfg["bien_etre"]["min_activities"])
    PRIME_PCT  = float(cfg["prime_sportive"]["percent"])
    MAX_WALK_KM = float(cfg["distance"]["max_walk_km"])
    MAX_BIKE_KM = float(cfg["distance"]["max_bike_km"])

    # Prime dynamique
    prime_df = (
        ref_rh
        .withColumn(
            "prime_sportive",
            (
                (F.col("moyen_dep") == "Marche/running") &
                (F.col("distance_km") <= F.lit(MAX_WALK_KM))
            ) |
            (
                (F.col("moyen_dep") == "VÃ©lo/Trottinette/Autres") &
                (F.col("distance_km") <= F.lit(MAX_BIKE_KM))
            )
        )
        .withColumn(
            "montant_prime_euros",
            F.when(F.col("prime_sportive"),
                   F.col("salaire_brut_annuel") * F.lit(PRIME_PCT)
            ).otherwise(0.0)
        )
        .withColumn("declaration_correcte", F.col("prime_sportive"))
        .select("employee_id", "bu", "prime_sportive", "montant_prime_euros", "distance_km", "declaration_correcte")
    )

    # ActivitÃ©s bien-Ãªtre
    well_df = (
        full_bronze.groupBy("employee_id")
        .agg(F.count("*").alias("nb_activites"))
        .withColumn("jours_bien_etre", F.col("nb_activites") >= MIN_ACTIVS)
    )

    # Fusion
    gold_df = (
        prime_df.join(well_df, on="employee_id", how="left")
        .fillna({"nb_activites": 0, "jours_bien_etre": False})
        .select("employee_id", "bu", "prime_sportive", "montant_prime_euros",
                "distance_km", "nb_activites", "jours_bien_etre", "declaration_correcte")
    )

    # Ã‰criture Gold
    gold_df.write.format("delta") \
        .mode("overwrite") \
        .option("overwriteSchema", "true") \
        .save(DELTA_GOLD_PATH)

    spark.sql(f"""
        CREATE TABLE IF NOT EXISTS gold_eligibilites
        USING DELTA
        LOCATION '{DELTA_GOLD_PATH}'
    """)

# â€”â€”â€” Bootstrap Bronze Delta â€”â€”â€”
log_dir = os.path.join(DELTA_BRONZE_PATH, "_delta_log")
timeout, interval, elapsed = 1000, 5, 0
print("â³ Jâ€™attends lâ€™initialisation de Bronzeâ€¦")
while True:
    if os.path.isdir(log_dir) and any(f.endswith(".json") for f in os.listdir(log_dir)):
        print("âœ… Bronze prÃªt.")
        break
    if elapsed > timeout:
        raise TimeoutError("Bronze pas initialisÃ© en Delta aprÃ¨s 1000â€¯s.")
    time.sleep(interval); elapsed += interval

# â€”â€”â€” âš¡ Recalcul initial Gold dÃ¨s le dÃ©marrage â€”â€”â€”
full_bronze_initial = spark.read.format("delta").load(DELTA_BRONZE_PATH)
if not full_bronze_initial.rdd.isEmpty():
    print("âš¡ Recalcul Gold initial Ã  partir de lâ€™historique Bronzeâ€¦")
    build_gold_from_bronze(full_bronze_initial, cfg)
else:
    print("âš ï¸ Bronze est vide, Gold sera construit lors du premier batch avec donnÃ©es.")

# â€”â€”â€” Streaming Bronze â€”â€”â€”
bronze_stream = (
    spark.readStream
         .format("delta")
         .load(DELTA_BRONZE_PATH)
)

# â€”â€”â€” Streaming : process_microbatch â€”â€”â€”
def process_microbatch(bronze_df, batch_id):
    global last_hash
    print(f"â–¶ï¸ Batch {batch_id} dÃ©marrÃ©")

    # Recharge config dynamique
    cfg, new_hash = load_config_and_hash()

    # Si config change â†’ suppression Gold avant recalcul
    if last_hash != new_hash:
        print(f"ğŸ”¥ Config changÃ©e â†’ suppression et reconstruction complÃ¨te Gold (batch {batch_id})")
        import shutil
        shutil.rmtree(DELTA_GOLD_PATH, ignore_errors=True)
        last_hash = new_hash
        with open(HASH_FILE, "w") as f:
            f.write(new_hash)

    # Lecture Bronze complet (mÃªme si batch vide) pour reconstruire Gold
    full_bronze = spark.read.format("delta").load(DELTA_BRONZE_PATH)
    if full_bronze.rdd.isEmpty():
        print(f"âš ï¸ Batch {batch_id} : Bronze complet vide, aucun calcul possible.")
        return

    build_gold_from_bronze(full_bronze, cfg)
    print(f"ğŸ’¾ Batch {batch_id} : Gold reconstruit.")

# â€”â€”â€” Lancement streaming â€”â€”â€”
query: StreamingQuery = (
    bronze_stream
    .writeStream
    .foreachBatch(process_microbatch)
    .outputMode("update")
    .trigger(processingTime="20 seconds")
    .option("checkpointLocation", CHECKPOINT_PATH)
    .start()
)

print("ğŸš€ GoldStreaming interactif avec recalcul immÃ©diat au dÃ©marrage et rebuild automatique en cas de changement config.yml.")
query.awaitTermination()
